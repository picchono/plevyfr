<!doctype html><html lang=fr><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5"><title>Gesture-based and Haptic Interfaces for Connected and Autonomous Driving | Pierre Lévy</title>
<link rel=stylesheet href=https://plevy.fr/css/epierrot.min.css media=screen><link rel=stylesheet href=https://plevy.fr/css/print.min.css media=print><script defer src=https://plevy.fr/js/epierrot.min.js></script><link href="https://api.fontshare.com/v2/css?f[]=amulya@700,701,400,401&display=swap" rel=stylesheet><meta property="og:title" content="Gesture-based and Haptic Interfaces for Connected and Autonomous Driving | Pierre Lévy"><meta property="og:type" content="article"><meta property="og:url" content="https://plevy.fr/fr/publications/gesture-based-and-haptic-interfaces-for-connected-and-autonomous-driving/"><meta property="og:description" content><meta property="og:locale" content="fr"><meta property="og:locale:alternate" content="en"><meta property="og:site_name" content="Pierre Lévy"><meta property="article:published_time" content="2016-07-31T15:17:29+02:00"><meta property="article:modified_time" content="2016-07-31T15:17:29+02:00"><meta property="article:section" content="publications"><body class=min-h-screen><div class="invisible h-0 w-0"><span data-pagefind-filter=Type>Publication</span>
<span data-pagefind-filter=Institution>TU/e</span></div><main class='w-full lg:w-auto' data-pagefind-body><div class="absolute z-50 top-3 left-3"><a class=text-primary-text href=https://plevy.fr><svg xmlns="http://www.w3.org/2000/svg" height="1.8rem" style="fill:currentColor;fill-rule:nonzero" class="bi bi-arrow-up-left-square-fill" height="1.8rem" viewBox="0 0 16 16"><path d="M2 0A2 2 0 000 2v12a2 2 0 002 2h12a2 2 0 002-2V2a2 2 0 00-2-2H2zm8.096 10.803L6 6.707v2.768a.5.5.0 01-1 0V5.5a.5.5.0 01.5-.5h3.975a.5.5.0 110 1H6.707l4.096 4.096a.5.5.0 11-.707.707z"/></svg></a></div><div class="w-full overflow-hidden flex flex-col md:flex-row"><div class="w-1/6 pl-5 pr-20 self-start"></div><div class="max-w-3xl w-full px-5"><div class="flex flex-row justify-start items-start w-full py-3.5 px-8 md:px-0"><div id=languageMode class='flex flex-row'><div class="inline text-sm"><b>FR</b>
|
<a href=https://plevy.fr/publications/gesture-based-and-haptic-interfaces-for-connected-and-autonomous-driving/ title=English rel=noreferrer>EN</a></div></div></div><div class="my-5 text-red-700 font-bold italic">Maintenance is currently in process... sorry for the trouble.</div><script src=/pagefind/pagefind-ui.js></script><div id=search></div><script>window.addEventListener("DOMContentLoaded",e=>{new PagefindUI({element:"#search",showSubResults:!0,highlightParam:"highlight",showEmptyFilters:!1})})</script><div class="text-left items-start"><h1 class="text-4xl font-bold">Gesture-based and Haptic Interfaces for Connected and Autonomous Driving</h1></div><div class=my-10><p>Terken, J., Lévy, P., Wang, W., Karjanto, J., Yusof, N.M.., Ros, F., & Zwaan, S. (2016). Gesture-Based and Haptic Interfaces for Connected and Autonomous Driving. In I.L., Nunes (Eds.) Advances in Human Factors and System Interactions, Proceedings of the AHFE 2016 International Conference on Human Factors and System Interactions, July 27-31, 2016, Walt Disney World®, Florida, USA (pp. 107-115). Switzerland: Springer International Publishing. http://dx.doi.org/10.1007/978-3-319-41956-5_11</p></div><div class="h-auto my-10"><p><b>/
Résumé
/</b>
While user interfaces for in-vehicle systems in the market are mostly button- and screen-based, advances in electronic technology provide designers with new design opportunities. In this paper, we propose applications of these novel technologies for several aspects of the current and future driving context. We explore opportunities for gesture-based and haptic interfaces in three different areas: establishing shared control between the driver and the autonomous vehicle; providing situation awareness to users of autonomous vehicles while engaged in other activities; connecting drivers to fellow drivers. We argue that these interface technologies hold the promise of creating richer and more natural interaction than the traditional vision- and audio-based interfaces that dominate the current market. We conclude by outlining steps for further research.</p></div><div class="max-w-3xl w-full px-5"><p class=my-10><a href='https://1drv.ms/b/s!AnQx_v88q65Qv4RCuOyYqlJbrPTRbA?e=ziwAC7' target=_blank class=doclinkButton rel=noreferrer><span class=doclinkIcon><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="file-pdf" class="svg-inline fa-file-pdf fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" height="1rem" viewBox="0 0 384 512"><path style="fill:currentColor;fill-rule:nonzero" d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5.0.0 21.5.0 48v416c0 26.5 21.5 48 48 48h288c26.5.0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5.0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4.0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg>
</span><span class=doclinkText>Article</span></a></p></div></div><div class="w-2/6 pl-20 pr-5">&nbsp;</div></div><div class="w-full overflow-hidden flex flex-col md:flex-row"><div class="w-1/6 pl-5 pr-20 self-end">&nbsp;</div><div class="max-w-3xl w-full px-5"></div><div class="w-2/6 pl-20 pr-5">&nbsp;</div></div><div class="w-full overflow-hidden flex flex-col md:flex-row mb-20"><div class="w-1/6 pl-5 pr-20 self-end">&nbsp;</div><div class="max-w-3xl w-full px-5"></div><div class="w-2/6 pl-20 pr-5">&nbsp;</div></div></main></body></html>